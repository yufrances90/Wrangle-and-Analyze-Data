{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "---\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Gathering](#gathering)\n",
    "* [Assessing](#assessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "\n",
    "# Introduction\n",
    "---\n",
    "\n",
    "> The goal of this project is wrangling WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. The Twitter archive is great, but it only contains very basic tweet information. Additional gathering, then assessing and cleaning is required for \"Wow!\"-worthy analyses and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "\n",
    "# Gathering\n",
    "---\n",
    "\n",
    "\n",
    "> **Enhanced Twitter Archive:** \n",
    ">\n",
    "> Provided by Udacity\n",
    "\n",
    "> **Additional Data via the Twitter API:**\n",
    ">\n",
    "> Used the tweet IDs in the WeRateDogs Twitter archive, queried the Twitter API for each tweet's JSON data using Python's Tweepy library and stored each tweet's entire set of JSON data in a file called tweet_json.txt file\n",
    "\n",
    "\n",
    "```python\n",
    "with open('twitter-credential.json') as f:\n",
    "    credentials = json.load(f)\n",
    "auth = tweepy.OAuthHandler(credentials['consumer_key'], credentials['consumer_secret'])\n",
    "auth.set_access_token(credentials['access_token'], credentials['access_token_secret'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "tweet_ids = df_wrd_twitter_old['tweet_id']\n",
    "statuses = []\n",
    "for tweet_id in tqdm(tweet_ids):\n",
    "    try:\n",
    "        status = api.get_status(tweet_id, tweet_mode='extended')\n",
    "        statuses.append(status._json)\n",
    "    except:\n",
    "        continue\n",
    "with open(json_filepath, 'w') as outfile:\n",
    "    for status in statuses:\n",
    "        json.dump(status, outfile)\n",
    "        outfile.write(\"\\n\")\n",
    "```\n",
    "\n",
    "\n",
    "> **Image Predictions File:**\n",
    ">\n",
    "> Downloaded programmatically using the Requests library and the following URL: [https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv)\n",
    "\n",
    "```python\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url, allow_redirects=True)\n",
    "with open(image_predictions_filepath, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessing'></a>\n",
    "\n",
    "# Assessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Key Points:**\n",
    "> * Only original ratings (no retweets) that have images is wanted\n",
    "> * The tweets beyond August 1st, 2017 do not need to be gathered.\n",
    "> * Cleaning includes merging individual pieces of data according to the rules of tidy data\n",
    "> * The rating numerators are greater than the denominators does not need to be cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Quality Issues:**\n",
    "> * IDs are found to have wrong types\n",
    "> * Data contains retweets\n",
    "> * Variable **retweeted** indicates whether this Tweet has been Retweeted by the authenticating user. Variable **retweet_count** indicates number of times this Tweet has been retweeted. The maxium of retweet_count reaches 77154 but the only possible value for retweeted is False\n",
    "> * The above issue is also found between **favourited** and **favorite_count**\n",
    "> * Misleading column names in df_image_predictions_old: p1, p1_conf, p1_dog, p2, p2_conf, p2_dog, p3, p3_conf, p3_dog\n",
    "> * Column name **text** is not descriptive\n",
    "> * Issues with columns p1, p2, p3\n",
    ">  * Not all words start with capitalized characters\n",
    ">  * Some uses \"_\", and some uses \"-\"\n",
    "> * **timestamp** is not of type DateTime. Same as **retweeted_status_timestamp**\n",
    "> * The **name** column in **df_wrd_twitter_old** contains words other than dog names\n",
    "> * The **rating_denominator** contains 0, but its corresponding **rating_numerator** is 960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
