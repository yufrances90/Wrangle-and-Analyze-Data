{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "---\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Gathering](#gathering)\n",
    "* [Assessing](#assessing)\n",
    "* [Cleaning](#cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "\n",
    "# Introduction\n",
    "---\n",
    "\n",
    "> The goal of this project is wrangling WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. The Twitter archive is great, but it only contains very basic tweet information. Additional gathering, then assessing and cleaning is required for \"Wow!\"-worthy analyses and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "\n",
    "# Gathering\n",
    "---\n",
    "\n",
    "\n",
    "> **Enhanced Twitter Archive** \n",
    ">\n",
    "> Provided by Udacity\n",
    "\n",
    "\n",
    "> **Image Predictions File**\n",
    ">\n",
    "> Downloaded programmatically using the Requests library and the following URL: [https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv)\n",
    "\n",
    "```python\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url, allow_redirects=True)\n",
    "with open(image_predictions_filepath, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "```\n",
    "\n",
    "> **Additional Data via the Twitter API**\n",
    ">\n",
    "> Used the tweet IDs in the WeRateDogs Twitter archive, queried the Twitter API for each tweet's JSON data using Python's Tweepy library and stored each tweet's entire set of JSON data in a file called tweet_json.txt file\n",
    "\n",
    "\n",
    "```python\n",
    "with open('twitter-credential.json') as f:\n",
    "    credentials = json.load(f)\n",
    "auth = tweepy.OAuthHandler(credentials['consumer_key'], credentials['consumer_secret'])\n",
    "auth.set_access_token(credentials['access_token'], credentials['access_token_secret'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "tweet_ids = df_wrd_twitter_old['tweet_id']\n",
    "statuses = []\n",
    "for tweet_id in tqdm(tweet_ids):\n",
    "    try:\n",
    "        status = api.get_status(tweet_id, tweet_mode='extended')\n",
    "        statuses.append(status._json)\n",
    "    except:\n",
    "        continue\n",
    "with open(json_filepath, 'w') as outfile:\n",
    "    for status in statuses:\n",
    "        json.dump(status, outfile)\n",
    "        outfile.write(\"\\n\")\n",
    "```\n",
    "\n",
    "> **Dog Breed Labels**\n",
    "> * Downloaded from Kaggle\n",
    "> * Did not programmatically download since the zip folder contains unnecessary files\n",
    "> * **URL**: https://www.kaggle.com/c/dog-breed-identification/data \n",
    "> * Only uses `labels.csv` in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessing'></a>\n",
    "\n",
    "# Assessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Key Points**\n",
    "> * Only original ratings (no retweets) that have images is wanted\n",
    "> * The tweets beyond August 1st, 2017 do not need to be gathered.\n",
    "> * Cleaning includes merging individual pieces of data according to the rules of tidy data\n",
    "> * The rating numerators are greater than the denominators does not need to be cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Enhanced Twitter Archive**\n",
    "> * name, doggo, floofer, pupper and puppo columns contain \"None\"\n",
    "> * name contains \"a\" as a value. May contain other nonsense name\n",
    "> * tweet information and dog information in the same table\n",
    "> * name (refers to dog name) is not very descriptive since tweet information and dog information are in the same table\n",
    "> * text (refers to status update) is not very descriptive since tweet information and dog information are in the same table\n",
    "> * All ids(tweet_id, in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id)\n",
    " have numeric data type instead of string\n",
    "> * timestamp is of string type\n",
    "> * Even though some columns have null values, it is reasonable\n",
    "> * doggo, floofer, pupper and puppo can be combined into a single column\n",
    "> * Columns may be ignored: \n",
    ">   in_reply_to_status_id, in_reply_to_user_id, source, expanded_urls\n",
    "> * Have invalid names other than 'None', eg: a, an, the, one. All starts with lowercase characters.\n",
    "> * Name column has duplicated entries\n",
    "> * There are 7 rows in df_wrd_twitter_old with wrong denominators and numerators\n",
    "> * There is 2 rows parsing time as ratings: 1) 24/7 2) 11/15/15\n",
    "> * name, doggo, pupper, puppo and floofer have \"None\" values as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Image Predictions File**\n",
    ">\n",
    "> * img_num, p1, p1_conf, p1_dog, etc. are not very descriptive\n",
    "> * Values of p1, p2, p3 have formatting issues. Some starts with uppercase characters, but others start with lowercase characters\n",
    "> * The data type of tweet_id is integer\n",
    "> * No missing values in this table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Additional Data via the Twitter API**\n",
    ">\n",
    "> * For each type of id, there are two columns for it for the same piece of information except that their data types are different\n",
    "> * Columns may be ignored: \n",
    ">   * created_at, truncated, in_reply_to_user_id, in_reply_to_status_id_str, in_reply_to_user_id\n",
    ">   * in_reply_to_user_id_str, in_reply_to_screen_name, is_quote_status, possibly_sensitive\n",
    ">   * possibly_sensitive_appealable, lang, quoted_status_id, quoted_status_id_str, quoted_status_permalink\n",
    ">   * quoted_status, geo, coordinates, place\n",
    "> * id is of numeric type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dog Breed Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "\n",
    "# Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
