{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "---\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Gathering](#gathering)\n",
    "* [Assessing](#assessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "\n",
    "# Introduction\n",
    "---\n",
    "\n",
    "> The goal of this project is wrangling WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. The Twitter archive is great, but it only contains very basic tweet information. Additional gathering, then assessing and cleaning is required for \"Wow!\"-worthy analyses and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "\n",
    "# Gathering\n",
    "---\n",
    "\n",
    "\n",
    "> **Enhanced Twitter Archive:** \n",
    ">\n",
    "> Provided by Udacity\n",
    "\n",
    "> **Additional Data via the Twitter API:**\n",
    ">\n",
    "> Used the tweet IDs in the WeRateDogs Twitter archive, queried the Twitter API for each tweet's JSON data using Python's Tweepy library and stored each tweet's entire set of JSON data in a file called tweet_json.txt file\n",
    "\n",
    "\n",
    "```python\n",
    "with open('twitter-credential.json') as f:\n",
    "    credentials = json.load(f)\n",
    "auth = tweepy.OAuthHandler(credentials['consumer_key'], credentials['consumer_secret'])\n",
    "auth.set_access_token(credentials['access_token'], credentials['access_token_secret'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "tweet_ids = df_wrd_twitter_old['tweet_id']\n",
    "statuses = []\n",
    "for tweet_id in tqdm(tweet_ids):\n",
    "    try:\n",
    "        status = api.get_status(tweet_id, tweet_mode='extended')\n",
    "        statuses.append(status._json)\n",
    "    except:\n",
    "        continue\n",
    "with open(json_filepath, 'w') as outfile:\n",
    "    for status in statuses:\n",
    "        json.dump(status, outfile)\n",
    "        outfile.write(\"\\n\")\n",
    "```\n",
    "\n",
    "\n",
    "> **Image Predictions File:**\n",
    ">\n",
    "> Downloaded programmatically using the Requests library and the following URL: [https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv)\n",
    "\n",
    "```python\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url, allow_redirects=True)\n",
    "with open(image_predictions_filepath, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessing'></a>\n",
    "\n",
    "# Assessing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
